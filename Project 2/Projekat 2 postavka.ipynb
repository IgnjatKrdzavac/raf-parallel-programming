{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Projekat2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MSThtR_v-3mc"},"source":["# Paralelni algoritmi - drugi projekat\n","\n","- Projekat se radi individualno ili u paru\n","\n","- Rok za predaju projekata je petak 17.12.2021.\n","\n","- Raspored odbrana će biti objavljen u ponedeljak 20.12.2021.\n","\n","- Predaja projekata je putem emaila, pri čemu programski kod bi trebalo da\n","bude prikačen uz sam email. \n","\n","- **Rešenje mora da bude obliku jedne python skripte (sa .py ekstenzijom), pri čemu ime fajla treba da bude ime_prezime_bbb.gg.py** (gde je bbb broj indeksa, a gg godina upisa).\n","\n","- Ukoliko dva studenta rade u paru, dovoljno je da jedan od njih preda, pri čemu šalje dva fajla (za svakoga poseban fajl, nazvan prema uputstvu) i \n","drugog stavlja u cc. E-mail treba da sadrzi sve podatke (ime, prezime,\n","broj indeksa) za oba studenta."]},{"cell_type":"markdown","metadata":{"id":"YdvScl8dGfnv"},"source":["# Zadatak\n","\n","U okviru projekta implementirate BPA tokenizator, koristeći MapReduce paradigmu. BPA tokenizator prvo analizira uzorak teksta kako bi identifikovao reči ili delove reči koji se često pojavljuju, a potom njih koristi kako bi tesktove pretvorio u vektore brojeva, koji se dalje mogu koristiti za statističke analize i mašinsko učenje.\n","Cilj projekta je uvežbavanje MapReduce programiranja, pa su neki zahtevi uprošćeni ili prilagođeni MapReduce kodu, dok će neki delovi verovatno biti previše spori za realnu upotrebu. \n","\n","## 1. Pristup tekstu sa Wikipedije putem MapReduce programa (3 boda)\n","Napisati program koji kroz map-reduce paradigmu (upotrebom funkcije `map` iz Python standardne biblioteke i funkcije `reduce` iz paketa `functools`):\n","  - Za zadatu listu ključnih reči vrši pretragu Wiki stranica i dohvata zadati broj rezultata (naslova stranica)\n","    Za pristup wiki stranicama se može koristiti python biblioteku [*wikipedia*](https://pypi.org/project/wikipedia/) (pip install wikipedia):\n","```python\n","import wikipedia\n","wikipedia.set_lang(\"sr\")  \n","_\n","def get_pages(query, results=50):\n","  '''Dohvata naslove zahtevanog broja stranica koje se pojavaljuju kao \n","      rezultati pretrage za zadatu kljucnu rec'''\n","      titles = wikipedia.search(query, results=results)\n","      pages = []\n","      for title in titles:\n","          page = wikipedia.page(title)\n","          pages.append(page)\n","      return pages\n","```\n","\n","- Neki naslovi koje ćemo dobiti pozivom `search` funkcije neće biti validni, pa će poziv `wikipedia.page(title)` baciti izuzetak. Izmeniti rešenje tako da se takvi naslovi odbace, bez propuštanja izuzetka. **0.5 bodova**\n","\n","- Pri kasnijoj obradi, biće nam zgodnije da umesto liste tesktova ima listu parova (ključ, tekst), gde je ključ ključna reč čijom pretragom smo došli do datog teksta. Modifikovati kod tako da vraća ovavu listu. **0.5 bodova**\n","\n","- Pomoću funkcije map i funkcije iz prethodne tačke napisati kod koji pretražuje za sledeći skup ključnih reči `['Beograd', 'Prvi svetski rat', 'Protein', 'Mikroprocesor', 'Stefan Nemanja', 'Košarka']`, za svaku dohvata po (maksimalno) 50 rezultata. Rezultat treba da bude lista, koja za svaku traženu ključnu reč sadrži listu ranije difinisanih `tuple`-ova (ključ, tekst). Izvršavanje može da potraje, a kako se uglavnom čeka na odgovor servera, može se ubrzati upotrebom map metoda iz pool-a koji koristi veći broj niti (`multiprocessing.pool.ThreadPool`) sa većim brojem (na primer 32) niti. **1 bod**\n","\n","- Pomoću funkcije reduce pretvoriti listu listi iz prethodne tačke u listu koja direktno sadrži sve tuplove (tj. spojiti sve \"unutrašnje\" liste u jednu, operacija koja se nekad naziva i `flatten`). **1 bod**\n","\n","\n","## 2. Pretprocesiranje teksta (3 boda)\n","\n","Pre dalje obrade potrebno je pripremiti tekst.\n","- Napisati kod koji pomoću funkcije `map` zamenjuje sva slova malim slovima. Kao ulaz očekivati tekst u obliku jednog niza karaktera (jedan `string`). Funkcija koja se mapira bi trebalo da radi na nivou jednog karaktera, i može koristiti python metod `str.lower`. Izalaz bi trebalo da bude u obliku liste karaktera. **0.5 bod**\n","\n","- Napisati kod koji pomoću funkcije `reduce` uklanja interpunkcijske znake i specijalne karaktere. Skup karaktera koji se uklanja možete sami odrediti, a najmanje treba da sadrži: `'=', '-', ',', '!', '?', '.', '$', '(', ')', '[', ']'`. Kao ulaz očekivati tekst u obliku liste karaktera. **1 bod**\n","\n","- Napisati kod koji pomoću funkcije `reduce` u listu karaktera dodaje jedan specijalni karakter posle svaki reči (graničnik ili \"terminator\" reči). Najbolje upotrebiti neki od karaktera koji su prethodno izbačeni iz teksta (tako smo sigurni da nije u pitanju karakter iz teksta). U praksi se često koristi karakter \"$\". Karaktere koji u originalnom tekstu označavaju krajeve reči (npr. \" \", \"\\n\", \"\\t\") ovom prilikom izbaciti. Graničnik na samom kraju liste je dozvoljeno dodati posebno, klasičnim kodom. **0.5 bod**\n","\n","- Kasniji koraci u izvršavanju će biti komputaciono zahtevni. Kako bi ogrnačili vreme izvršavanja, dobro je da ograničimo i dužinu teksta sa kojim radimo. Napisati kod koji pomoću funkcije `reduce` niz iz prethodne tačke skraćuje na otprilike 10.000 karaktera, tako da se uvek završrava graničnikom reči (na primer, kad se stigne do 10.000 karaktera nastavlja se do sledećeg graničnika i tu staje. **0.5 bodova**\n","\n","- Napisati funkciju koja za dati tekst poziva prethodna 4 koraka pretprocesiranja, i vraća obrađenu listu karaktera. Pomoću funkcije `map`, primeniti ovu funkciju na listu tekstova sa wikipedia-e, iz prvog dela zadataka. Rezultat bi trebalo da bude u vidu liste parova (ključ, niz karaktera), gde je ključ ključna reč čijom pretragom smo došli do datog teksta. **0.5 bodova**\n","\n","\n","## 3. Kreiranje BPA tokenizatora (6 bodova)\n","\n","- Tokenizator ćemo kreirati (\"trenirati\") na manjem podskupu tesktova koje smo pripremili.   \n","Pomoću funkcije reduce, napisati kod koji iz prethodne stavke nasumično odabira po 5 tesktova iz za svaku pretraženu ključnu reč (prihvatljivo je da to budu i prvih 5 po redosledu kojim su dati u listi, ukoliko usmeno možete da skicirate rešenje koje bi bilo bliže nasumičnom odabiranju). Rezultate bi trebalo da bude *jedna prosta lista karaktera* (odbacujemo ključeve, i sve spajamo u jednu listu). **1 bod**\n","\n","Sada ćemo napisati kod koji izvršava jednu iteraciju treniranja BPA tokenizatora.\n","\n","- Pomoću funkcije `reduce`, napisati kod koji za datu listu generiše listu kandata za sledeći token koji će biti dodat u listu tokena, tako što za svaki par x<sub>i</sub>, x<sub>i+1</sub> emituje njihovu konkatenaciju. Na primer, za niz `['A', 'B', 'C', 'A', 'B']` rezultat bi bio `['AB', 'BC', 'CA', 'AB']`. Voditi računa da se graničnik reči može pronaći samo na poslednjem mestu u tekstu tokena (Na primer, za ulaz `['A', '$', 'B', 'C']` rezultat bi trebalo da bude `['A$', 'BC']`). **1 bod**\n","\n","- Pomoću funkcija `map`, `sorted`, i `reduce` (dozvoljeno je korišćenje python funkcije sorted između map i reduce faza) napisati koji broji pojavljivanja pojednih kandandata za sledeći token, u obliku niza tuplova (token, broj pojavljivanja). Na primer, za niz `['AB', 'BC', 'CA', 'AB']` rezultat bi bio `[('AB', 2), ('BC', 1), ('CA', 1)]`. **2 boda**\n","\n","- Pomoću funkcije `sorted` sortirati niz iz prethodne tačke po broju pojavljivanja tokena, uzeti najčešći, i dodati ga u trenutnu listu tokena.  \n","*Klasični BPA tokenizator polazi od liste slova, dok mi možemo krenuti od prazne liste. Takođe klasični BPA će kod dodavanja tokena smanjiti brojače tokena čijim je spajanjem nastao, a one čiji je broj stigao do 0 izbaciti; ni o ovome ne moramo voditi računa, pošto smo dodali korak na kraju koji rešava oba problema po cenu dodatnog računanja*\n","\n","- Pomoću funkcije `reduce` proći kroz listu karaktera koju koristimo pri za treniranje i ubaciti novi token gde god se pojavljuje. Na primer, ako ubacujemo token 'AB' u listu `['A', 'B', 'C', 'A', 'B']` rezultat bi trebalo da bude `['AB', 'C', 'AB']`. Dobijena lista će biti ulaz u sledeću iteraciju.\n","U kasnijim iteracijama ubačene tokene tretiramo isto kao i ostale karaktere; na primer za ulaz `['AB', 'C', 'AB']` prvi korak generiše `['ABC', 'CAB']` i tako dalje. **2 boda**\n","\n","- Klasičnom **for** petljom izvršiti 5000 iteracija treniranja BPE tokenizatora.   \n"," **Ovaj korak može biti spor, u zavisnosti od toga koliko je optimalno napisan kod, od 15 minuta pa na više.** Dok razvijate kod, možete spustiti broj iteracija na 10-50. Rezultat će biti uglavnom kratki tokeni, dok se posle par hiljada iteracija počinju pojavljivati cele reči (praktični BPE tokenizatori se često treniraju 50.000+ iteracija, ali je i njhov kod optimalnije napisan). \n","\n","- Pomoću funkcija `map`, `sorted`, i `reduce`  (dozvoljeno je korišćenje python funkcije sorted između map i reduce faza) napisati kod koji broji pojavljivanja pojednih elemenata u listi dobijenoj transformacijom ulaznog teksta kroz iteracija treniranja.   \n","Elementi ulazne liste su tokeni i pojednični karakteri koji do sada nisu zamenjeni tokenima. Pojedinačne karaktere tretirati isto kao i tokene (u ovom koraku ćemo ih praktično dodati u listu tokena).  \n","Rezultat bi trebalo da bude lista u obliku (token, broj pojavljivanja). **1 bod**\n","\n","- Sortirati listu iz prethodne stavke po dužini tokena. Ova lista nam konačni skup tokena u tokenizatoru (primetite da smo na ovaj način i odbacili sve tokene koji su dodati u procesu treniranja, a više se ne pojavljuju u tekstu, pošto su uključeni u duže tokene).\n","\n","## 4. Primena BPA tokenizatora (8 bodova)\n","\n","U ovom delu zadatka se vraćamu na punu listu pretprocesiranih tekstova u obliku liste parova (ključ, lista karaktera) iz prvog dela zadatka.\n","\n","- Pomoću funkcije `reduce` napisati kod koji prolazi kroz listu karaktera (i potencijalno već ubačenih tokena) i token, i ubacije dati token u izlaznu listu gde god se on pojavljuje. Ova funkcija može zameniti samo sukcesivne karaktere novim tokenom (već ubačeni tokeni ne mogu biti zamenjeni). \n","Kod bi trebalo da bude u obliku funkcije koja na ulazu prima listu već obrađenih elemenata ulaza (eventualno u tuple-e sa nekim brojačem ili flag promenljivom), sledeći karakter iz ulazen liste, i token koji se trenutno obradjuje. Upotrebom funckije `partial` iz paketa `functools` ova funkcija bi trebalo da može da se pretvori u funckiju koja je kompatibilna sa reduce. **2 boda**\n","```python\n","def insert_token(accumulator, value, token):\n","...\n","inserter = partial(insert_token, token=token)\n","tokenized = reduce(inserter, tokenized, ([], 0))\n","```\n","  **Kod implementacije ove funkcije je potreno voditi računa o kompleksnoti, pošto ćemo je izvršavati za svaki tekst i za svaki token, a sama funkcija iterira kroz tekst slovo po slovo.** Jednom kada ste sigurni da radi, možete joj dodati dekorator `@jit` dekorator iz `numba` paketa, kako bi dobili znatno brži, kompajlirani kod. Bez numbe, obrara može potrejati i preko 1h (ili mnogo sati), u zavisnosti od implementacije.\n","\n","- Pomoću funckije `map` primeniti funkciju iz prethodne tačne na sve ulazne tekstove. Rezultat bi trebalo da bude u vidu liste parova (ključ, niz tokena), gde je ključ ključna reč čijom pretragom smo došli do datog teksta. Izvršavanje može da potraje, pa je dobro ovaj map zameniti map-om koji radi sa više procesa (na Colabu samo 2 procesa, pošto imamo samo dva jezgra). **1 bod**\n","\n","- Pomoću funkcija `map` i `reduce` izbrojati pojavljivanje **svih** tokena iz liste tokena, u svakom od tekstova. Rezultat treba da bude u vidu liste parova (ključ, lista broja pojavlivanja), pri čemu:\n","  - Svakom ulaznom tekstu odgovara jedan element liste\n","  - Ključevi su ključne reči čijom pretragom smo dobili dati tekst\n","  - Lista broja tokena treba da sadrži sve token (0 za one koji se pojavljuju u datom tekstu), i mora da bude isto sortirana za sve tekstove (ako prvi broj u nizu za prvi tekst označava koliko se puta reč \"grad\" pojavila u prvom tekstu, onda prvi broj u listama svih ostalih tekstova takođe označava koliko se puta reč \"grad\" pojavila u datom tekstu). **4 boda**  \n","  Moguća ideja za rešenje (za jedan tekst) se bazira na map/reduce implementaciji `join`-a:\n","    - Metodom `map` pretvoriti listu tokena u listu parova ('A', (token, 0)). (ili poći od list tokena koja sadrži broj pojavljivaja tokena u toku treniranja, u kom slučaju broj 0 ne moramo da dodajemo)\n","\n","    - Pomoću funkcija `map`, `sorted`, i `reduce` napisati kod koji broji pojavljivanja tokena u tokenizovanom tekstu Rezultat treba da bude u vidu liste parova [token, broj pojavljivanja], (tokeni koji se ne pojavljuju u tekstu ne neće pojaviti u rezultatu, redosled tokena je nasumičan). \n","\n","    - Metodom `map` pretvoriti listu iz prethodne stavke u listu parova ('B', (token, N)), gde je N broj pojavljivanja datog tokena.\n","\n","    - Spojiti liste iz prve i treće stavke (list_a + list_b) i sortirati dobijenu listu po vrednosti tokena, pa po labeli (\"A\" ili \"B\"). Rezultat je lista koja sadrži:\n","      - Svaki token iz liste tokena, praćen brojem pojavljivanja u datom tekstu, kao susedne elemente\n","      - Tokene koji nisu u pronadjeni u tekstu, praćene nečim drugim\n","      - Karaktere pronadjene u tekstu koji nisu u listi tokena, ispred kojih se neće nalaziti odgovorajući toke\n","    - Pomoću funkcije reduce proći kroz ovakvu listu i formirati traženi rezultat.\n","\n","- Pomoće metode `map` primeniti kod koji vrši tokenizaciju na sve tekstove. **1 bod**"]},{"cell_type":"markdown","metadata":{"id":"EaZAUWrhEVo2"},"source":["## Klasterovanje\n","(Ne nosi bodove, nije neophodno)\n","\n","Kod dat u nastavku mozete iskoristiti za evaluaciju dobijenih embedinga tekstova. Kod očekuje list bpr, čiji su elementi parovi (klučna reč, bpr tokenizovan tekst) i vrši klasterovanjem k-means algoritmom u 6 klastera. *Kako je priprema teksta relativno jednostavna, ne očekuju se savršeni rezultati*."]},{"cell_type":"code","metadata":{"id":"1OGQr0VX1Qfb"},"source":["from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","import numpy as np\n","from collections import Counter\n","\n","labels = [x[0] for x in bpes]\n","embeddings = np.array([x[1] for x in bpes])\n","\n","pca_embeddings = PCA(n_components=50).fit_transform(embeddings)\n","clusters = KMeans(n_clusters=6).fit_predict(pca_embeddings)\n","Counter(zip(labels, clusters))"],"execution_count":null,"outputs":[]}]}